<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en">

<!-- Mirrored from arxiv.org/list/stat/new by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 29 Mar 2019 22:40:35 GMT -->
<!-- Added by HTTrack --><meta http-equiv="content-type" content="text/html;charset=utf-8" /><!-- /Added by HTTrack -->
<head>
<title>Statistics  authors/titles &quot;new&quot;</title>
<link rel="shortcut icon" href="../../favicon.ico" type="image/x-icon" />
<link rel="stylesheet" type="text/css" media="screen" href="../../css/arXivfecf.css?v=20190306" />
<link rel="stylesheet" type="text/css" media="screen" href="../../bibex/bibex2785.css?20181009">
<link rel="stylesheet" href="../../../maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css"> 
<link rel="alternate" type="application/rss+xml" title="Statistics " href="http://arxiv.org/rss/stat"/>
<script src="https://arxiv.org/js/mathjaxToggle.min.js" type="text/javascript"></script>

<!-- Piwik -->
<script type="text/javascript">
var _paq = _paq || [];
_paq.push(["setDomains", ["*.arxiv.org"]]);
_paq.push(['trackPageView']);
_paq.push(['enableLinkTracking']);
(function()
{ var u="../../../webanalytics.library.cornell.edu/index.html"; _paq.push(['setTrackerUrl', u+'piwik.php']); _paq.push(['setSiteId', 538]); var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript'; g.async=true; g.defer=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s); }
)();
</script>
<!-- End Piwik Code -->


</head>
<body class="with-cu-identity">

<noscript><img src="../../../webanalytics.library.cornell.edu/piwikf25d.gif?idsite=538&amp;rec=1" style="border:0;" alt="" /></noscript>

<div id="cu-identity">
<div id="cu-logo">
<a href="../../../www.cornell.edu/index.html"><img src="../../../static.arxiv.org/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" border="0" /></a>
</div>
<div id="support-ack">
<a href="https://confluence.cornell.edu/x/ALlRF">We gratefully acknowledge support from<br /> the Simons Foundation and member institutions.</a>
</div>
</div>
<div id="header">
<h1><a href="../../index.html">arXiv.org</a> &gt; <a href="recent.html">stat</a></h1>
<div id="search">
<form id="search-arxiv" method="GET" action="https://arxiv.org/search">
                
<div class="wrapper-search-arxiv">
<input class="keyword-field" type="text" name="query" placeholder="Search or Article ID"/>

<div class="filter-field">
<select name="searchtype">
<option value="all">All fields</option>
<option value="title">Title</option>
<option value="author">Author(s)</option>
<option value="abstract">Abstract</option>
<option value="comments">Comments</option>
<option value="journal_ref">Journal reference</option>
<option value="acm_class">ACM classification</option>
<option value="msc_class">MSC classification</option>
<option value="report_num">Report number</option>
<option value="paper_id">arXiv identifier</option>
<option value="doi">DOI</option>
<option value="orcid">ORCID</option>
<option value="author_id">arXiv author ID</option>
<option value="help">Help pages</option>
<option value="full_text">Full text</option></select>
</div>
<input class="btn-search-arxiv" value="" type="submit">
<div class="links">(<a href="../../help.html">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced search</a>)</div>
</div>
<input type="hidden" name="source" value="header">
</form>
</div>
</div>
<div id="content">
<div id="dlpage">
<h1>Statistics </h1>
<h2>New submissions</h2>
<div class="list-dateline">Submissions received from  Wed 27 Mar 19  to  Thu 28 Mar 19, announced Fri, 29 Mar 19</div>
<ul>
<li><a href="https://arxiv.org/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item20">Cross-lists</a></li>
<li><a href="#item47">Replacements</a></li>
</ul>
<small>[ total of 77 entries:  <b>1-77</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="https://arxiv.org/list/stat/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
<h3>New submissions for Fri, 29 Mar 19</h3>
<dl>
<dt><a name="item1">[1]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11647" title="Abstract">arXiv:1903.11647</a> [<a href="https://arxiv.org/pdf/1903.11647" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate Bayesian inference for multivariate point pattern analysis  in disease mapping
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Palmi-Perales%2C+F">Francisco Palmi-Perales</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Gomez-Rubio%2C+V">Virgilio Gomez-Rubio</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Lopez-Abente%2C+G">Gonzalo Lopez-Abente</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Ramis-Prieto%2C+R">Rebeca Ramis-Prieto</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Sanz-Anquela%2C+J+M">Jose Miguel Sanz-Anquela</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Fernandez-Navarro%2C+P">Pablo Fernandez-Navarro</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
<p class="mathjax">We present a novel approach for the analysis of multivariate case-control
georeferenced data using Bayesian inference in the context of disease mapping,
where the spatial distribution of different types of cancers is analyzed.
Extending other methodology in point pattern analysis, we propose a
log-Gaussian Cox process for point pattern of cases and the controls, which
accounts for risk factors, such as exposure to pollution sources, and includes
a term to measure spatial residual variation.
<br />For each disease, its intensity is modeled on a baseline spatial effect
(estimated from both controls and cases), a disease-specific spatial term and
the effects on covariates that account for risk factors. By fitting these
models the effect of the covariates on the set of cases can be assessed, and
the residual spatial terms can be easily compared to detect areas of high risk
not explained by the covariates.
<br />Three different types of effects to model exposure to pollution sources are
considered. First of all, a fixed effect on the distance to the source. Next,
smooth terms on the distance are used to model non-linear effects by means of a
discrete random walk of order one and a Gaussian process in one dimension with
a Mat\'ern covariance.
<br />Models are fit using the integrated nested Laplace approximation (INLA) so
that the spatial terms are approximated using an approach based on solving
Stochastic Partial Differential Equations (SPDE). Finally, this new framework
is applied to a dataset of three different types of cancer and a set of
controls from Alcal\'a de Henares (Madrid, Spain). Covariates available include
the distance to several polluting industries and socioeconomic indicators. Our
findings point to a possible risk increase due to the proximity to some of
these industries.
</p>
</div>
</dd>
<dt><a name="item2">[2]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11683" title="Abstract">arXiv:1903.11683</a> [<a href="https://arxiv.org/pdf/1903.11683" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11683" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Outlier-Robust Spatial Perception: Hardness, General-Purpose Algorithms,  and Guarantees
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Tzoumas%2C+V">Vasileios Tzoumas</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Antonante%2C+P">Pasquale Antonante</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Carlone%2C+L">Luca Carlone</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG); Robotics (cs.RO); Systems and Control (cs.SY); Applications (stat.AP)

</div>
<p class="mathjax">Spatial perception is the backbone of many robotics applications, and spans a
broad range of research problems, including localization and mapping, point
cloud alignment, and relative pose estimation from camera images. Robust
spatial perception is jeopardized by the presence of incorrect data
association, and in general, outliers. Although techniques to handle outliers
do exist, they can fail in unpredictable manners (e.g., RANSAC, robust
estimators), or can have exponential runtime (e.g., branch-and-bound). In this
paper, we advance the state of the art in outlier rejection by making three
contributions. First, we show that even a simple linear instance of outlier
rejection is inapproximable: in the worst-case one cannot design a
quasi-polynomial time algorithm that computes an approximate solution
efficiently. Our second contribution is to provide the first per-instance
sub-optimality bounds to assess the approximation quality of a given outlier
rejection outcome. Our third contribution is to propose a simple
general-purpose algorithm, named adaptive trimming, to remove outliers. Our
algorithm leverages recently-proposed global solvers that are able to solve
outlier-free problems, and iteratively removes measurements with large errors.
We demonstrate the proposed algorithm on three spatial perception problems: 3D
registration, two-view geometry, and SLAM. The results show that our algorithm
outperforms several state-of-the-art methods across applications while being a
general-purpose method.
</p>
</div>
</dd>
<dt><a name="item3">[3]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11695" title="Abstract">arXiv:1903.11695</a> [<a href="https://arxiv.org/pdf/1903.11695" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11695" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Multinomial Logistic Normal Models through Marginally Latent  Matrix-T Processes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Silverman%2C+J+D">Justin D. Silverman</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Roche%2C+K">Kimberly Roche</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Holmes%2C+Z+C">Zachary C. Holmes</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=David%2C+L+A">Lawrence A. David</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
<p class="mathjax">Bayesian multinomial logistic-normal (MLN) models are popular for the
analysis of sequence count data (e.g., microbiome or gene expression data) due
to their ability to model multivariate count data with complex covariance
structure. However, existing implementations of MLN models are limited to
handling small data sets due to the non-conjugacy of the multinomial and
logistic-normal distributions. We introduce MLN models which can be written as
marginally latent matrix-t process (LTP) models. Marginally LTP models describe
a flexible class of generalized linear regression, non-linear regression, and
time series models. We develop inference schemes for Marginally LTP models and,
through application to MLN models, demonstrate that our inference schemes are
both highly accurate and often 4-5 orders of magnitude faster than MCMC.
</p>
</div>
</dd>
<dt><a name="item4">[4]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11696" title="Abstract">arXiv:1903.11696</a> [<a href="https://arxiv.org/pdf/1903.11696" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11696" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Stable prediction with radiomics data
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Peeters%2C+C+F+W">Carel F.W. Peeters</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=%C3%9Cbelh%C3%B6r%2C+C">Caroline &#xdc;belh&#xf6;r</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Mes%2C+S+W">Steven W. Mes</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Martens%2C+R">Roland Martens</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Koopman%2C+T">Thomas Koopman</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=de+Graaf%2C+P">Pim de Graaf</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=van+Velden%2C+F+H+P">Floris H.P. van Velden</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Boellaard%2C+R">Ronald Boellaard</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Castelijns%2C+J+A">Jonas A. Castelijns</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Beest%2C+D+E+t">Dennis E. te Beest</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Heymans%2C+M+W">Martijn W. Heymans</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=van+de+Wiel%2C+M+A">Mark A. van de Wiel</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 52 pages: 14 pages Main Text and 38 pages of Supplementary Material
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG); Image and Video Processing (eess.IV); Quantitative Methods (q-bio.QM); Applications (stat.AP); Methodology (stat.ME)

</div>
<p class="mathjax">Motivation: Radiomics refers to the high-throughput mining of quantitative
features from radiographic images. It is a promising field in that it may
provide a non-invasive solution for screening and classification. Standard
machine learning classification and feature selection techniques, however, tend
to display inferior performance in terms of (the stability of) predictive
performance. This is due to the heavy multicollinearity present in radiomic
data. We set out to provide an easy-to-use approach that deals with this
problem.
<br />Results: We developed a four-step approach that projects the original
high-dimensional feature space onto a lower-dimensional latent-feature space,
while retaining most of the covariation in the data. It consists of (i)
penalized maximum likelihood estimation of a redundancy filtered correlation
matrix. The resulting matrix (ii) is the input for a maximum likelihood factor
analysis procedure. This two-stage maximum-likelihood approach can be used to
(iii) produce a compact set of stable features that (iv) can be directly used
in any (regression-based) classifier or predictor. It outperforms other
classification (and feature selection) techniques in both external and internal
validation settings regarding survival in squamous cell cancers.
</p>
</div>
</dd>
<dt><a name="item5">[5]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11697" title="Abstract">arXiv:1903.11697</a> [<a href="https://arxiv.org/pdf/1903.11697" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11697" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bayesian Experimental Design for Oral Glucose Tolerance Tests (OGTT)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kuschinski%2C+N+E">Nicol&#xe1;s E. Kuschinski</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Christen%2C+J+A">J. Andr&#xe9;s Christen</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Monroy%2C+A">Adriana Monroy</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Alavez%2C+S">Silvestre Alavez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 5 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">OGTT is a common test, frequently used to diagnose insulin resistance or
diabetes, in which a patient's blood sugar is measured at various times over
the course of a few hours. Recent developments in the study of OGTT results
have framed it as an inverse problem which has been the subject of Bayesian
inference. This is a powerful new tool for analyzing the results of an OGTT
test,and the question arises as to whether the test itself can be improved. It
is of particular interest to discover whether the times at which a patient's
glucose is measured can be changed to improve the effectiveness of the test.
The purpose of this paper is to explore the possibility of finding a better
experimental design, that is, a set of times to perform the test. We review the
theory of Bayesian experimental design and propose an estimator for the
expected utility of a design. We then study the properties of this estimator
and propose a new method for quantifying the uncertainty in comparisons between
designs. We implement this method to find a new design and the proposed design
is compared favorably to the usual testing scheme.
</p>
</div>
</dd>
<dt><a name="item6">[6]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11745" title="Abstract">arXiv:1903.11745</a> [<a href="https://arxiv.org/pdf/1903.11745" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11745" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Approximate spectral gaps for Markov chains mixing times in  high-dimension
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Atchad%C3%A9%2C+Y+F">Yves F. Atchad&#xe9;</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 34 pages 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>

</div>
<p class="mathjax">This paper introduces a concept of approximate spectral gap to analyze the
mixing time of Markov Chain Monte Carlo (MCMC) algorithms for which the usual
spectral gap is degenerate or almost degenerate. We use the idea to analyze a
class of MCMC algorithms to sample from mixtures of densities. As an
application we study the mixing time of a popular Gibbs sampler for variable
selection in linear regression models. Under some regularity conditions on the
signal and the design matrix of the regression problem, we show that for
well-chosen initial distributions the mixing time of the Gibbs sampler is
polynomial in the dimensional of the space.
</p>
</div>
</dd>
<dt><a name="item7">[7]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11865" title="Abstract">arXiv:1903.11865</a> [<a href="https://arxiv.org/pdf/1903.11865" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11865" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Correlating Paleoclimate Time Series: Sources of Uncertainty and  Potential Pitfalls
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Franke%2C+J+G">Jasper G. Franke</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Donner%2C+R+V">Reik V. Donner</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
<p class="mathjax">Comparing paleoclimate time series is complicated by a variety of typical
features, including irregular sampling, age model uncertainty (e.g., errors due
to interpolation between radiocarbon sampling points) and time uncertainty
(uncertainty in calibration), which, taken together, result in unequal and
uncertain observation times of the individual time series to be correlated.
Several methods have been proposed to approximate the joint probability
distribution needed to estimate correlations, most of which rely either on
interpolation or temporal downsampling.
<br />Here, we compare the performance of some popular approximation methods using
synthetic data resembling common properties of real world marine sediment
records. Correlations are determined by estimating the parameters of a
bivariate Gaussian model from the data using Markov Chain Monte Carlo sampling.
We complement our pseudoproxy experiments by applying the same methodology to a
pair of marine benthic oxygen records from the Atlantic Ocean.
<br />We find that methods based upon interpolation yield better results in terms
of precision and accuracy than those which reduce the number of observations.
In all cases, the specific characteristics of the studied time series are,
however, more important than the choice of a particular interpolation method.
Relevant features include the number of observations, the persistence of each
record, and the imposed coupling strength between the paired series. In most of
our pseudoproxy experiments, uncertainty in observation times introduces less
additional uncertainty than unequal sampling and errors in observation times
do. Thus, it can be reasonable to rely on published time scales as long as
calibration uncertainties are not known.
</p>
</div>
</dd>
<dt><a name="item8">[8]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11867" title="Abstract">arXiv:1903.11867</a> [<a href="https://arxiv.org/pdf/1903.11867" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.11867" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.11867" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Classification of sparse binary vectors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Chzhen%2C+E">Evgenii Chzhen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>

</div>
<p class="mathjax">In this work we consider a problem of multi-label classification, where each
instance is associated with some binary vector. Our focus is to find a
classifier which minimizes false negative discoveries under constraints.
Depending on the considered set of constraints we propose plug-in methods and
provide non-asymptotic analysis under margin type assumptions. Specifically, we
analyze two particular examples of constraints that promote sparse predictions:
in the first one, we focus on classifiers with $\ell_0$-type constraints and in
the second one, we address classifiers with bounded false positive discoveries.
Both formulations lead to different Bayes rules and, thus, different plug-in
approaches. The first considered scenario is the popular multi-label top-$K$
procedure: a label is predicted to be relevant if its score is among the $K$
largest ones. For this case, we provide an excess risk bound that achieves so
called `fast' rates of convergence under a generalization of the margin
assumption to this settings. The second scenario differs significantly from the
top-$K$ settings, as the constraints are distribution dependent. We demonstrate
that in this scenario the almost sure control of false positive discoveries is
impossible without extra assumptions. To alleviate this issue we propose a
sufficient condition for the consistent estimation and provide non-asymptotic
upper-bound.
</p>
</div>
</dd>
<dt><a name="item9">[9]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11886" title="Abstract">arXiv:1903.11886</a> [<a href="https://arxiv.org/pdf/1903.11886" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11886" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regression-based Network Reconstruction with Nodal and Dyadic Covariates  and Random Effects
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Lebacher%2C+M">Michael Lebacher</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kauermann%2C+G">G&#xf6;ran Kauermann</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 35 pages, 7 figures, 4 tables
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
<p class="mathjax">Network (or matrix) reconstruction is a general problem which occurs if the
margins of a matrix are given and the matrix entries need to be predicted. In
this paper we show that the predictions obtained from the iterative
proportional fitting procedure (IPFP) or equivalently maximum entropy (ME) can
be obtained by restricted maximum likelihood estimation relying on augmented
Lagrangian optimization. Based on the equivalence we extend the framework of
network reconstruction towards regression by allowing for exogenous covariates
and random heterogeneity effects. The proposed estimation approach is compared
with different competing methods for network reconstruction and matrix
estimation. Exemplary, we apply the approach to interbank lending data,
provided by the Bank for International Settlement (BIS). This dataset provides
full knowledge of the real network and is therefore suitable to evaluate the
predictions of our approach. It is shown that the inclusion of exogenous
information allows for superior predictions in terms of $L_1$ and $L_2$ errors.
Additionally, the approach allows to obtain prediction intervals via bootstrap
that can be used to quantify the uncertainty attached to the predictions.
</p>
</div>
</dd>
<dt><a name="item10">[10]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11907" title="Abstract">arXiv:1903.11907</a> [<a href="https://arxiv.org/pdf/1903.11907" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11907" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Meta-Learning surrogate models for sequential decision making
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Galashov%2C+A">Alexandre Galashov</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Schwarz%2C+J">Jonathan Schwarz</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kim%2C+H">Hyunjik Kim</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Garnelo%2C+M">Marta Garnelo</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Saxton%2C+D">David Saxton</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kohli%2C+P">Pushmeet Kohli</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Eslami%2C+S+M+A">S.M. Ali Eslami</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Teh%2C+Y+W">Yee Whye Teh</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Meta-learning methods leverage past experience to learn data-driven inductive
biases from related problems, increasing learning efficiency on new tasks. This
ability renders them particularly suitable for sequential decision making with
limited experience. Within this problem family, we argue for the use of such
approaches in the study of model-based approaches to Bayesian Optimisation,
contextual bandits and Reinforcement Learning. We approach the problem by
learning distributions over functions using Neural Processes (NPs), a recently
introduced probabilistic meta-learning method. This allows the treatment of
model uncertainty to tackle the exploration/exploitation dilemma. We show that
NPs are suitable for sequential decision making on a diverse set of domains,
including adversarial task search, recommender systems and model-based
reinforcement learning.
</p>
</div>
</dd>
<dt><a name="item11">[11]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11908" title="Abstract">arXiv:1903.11908</a> [<a href="https://arxiv.org/pdf/1903.11908" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.11908" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.11908" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Generalizing the Balance Heuristic Estimator in Multiple Importance  Sampling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Elvira%2C+M+S+u+V">Mateu Sbert un V&#xed;ctor Elvira</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>

</div>
<p class="mathjax">In this paper, we propose a novel and generic family of multiple importance
sampling estimators. We first revisit the celebrated balance heuristic
estimator, a widely used Monte Carlo technique for the approximation of
intractable integrals. Then, we establish a generalized framework for the
combination of samples simulated from multiple proposals. We show that the
novel framework contains the balance heuristic as a particular case. In
addition, we study the optimal choice of the free parameters in such a way the
variance of the resulting estimator is minimized. A theoretical variance study
shows the optimal solution is always better than the balance heuristic
estimator (except in degenerate cases where both are the same). As a side
result of this analysis, we also provide new upper bounds for the balance
heuristic estimator. Finally, we show the gap in the variance of both
estimators by means of five numerical examples.
</p>
</div>
</dd>
<dt><a name="item12">[12]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11990" title="Abstract">arXiv:1903.11990</a> [<a href="https://arxiv.org/pdf/1903.11990" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11990" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> On the Stability and Generalization of Learning with Kernel Activation  Functions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Cirillo%2C+M">Michele Cirillo</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Scardapane%2C+S">Simone Scardapane</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Van+Vaerenbergh%2C+S">Steven Van Vaerenbergh</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Uncini%2C+A">Aurelio Uncini</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted as a brief paper to IEEE TNNLS
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">In this brief we investigate the generalization properties of a
recently-proposed class of non-parametric activation functions, the kernel
activation functions (KAFs). KAFs introduce additional parameters in the
learning process in order to adapt nonlinearities individually on a per-neuron
basis, exploiting a cheap kernel expansion of every activation value. While
this increase in flexibility has been shown to provide significant improvements
in practice, a theoretical proof for its generalization capability has not been
addressed yet in the literature. Here, we leverage recent literature on the
stability properties of non-convex models trained via stochastic gradient
descent (SGD). By indirectly proving two key smoothness properties of the
models under consideration, we prove that neural networks endowed with KAFs
generalize well when trained with SGD for a finite number of steps.
Interestingly, our analysis provides a guideline for selecting one of the
hyper-parameters of the model, the bandwidth of the scalar Gaussian kernel. A
short experimental evaluation validates the proof.
</p>
</div>
</dd>
<dt><a name="item13">[13]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12012" title="Abstract">arXiv:1903.12012</a> [<a href="https://arxiv.org/pdf/1903.12012" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Forecasting model based on information-granulated GA-SVR and ARIMA for  producer price index
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Tang%2C+X">Xiangyan Tang</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wang%2C+L">Liang Wang</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Cheng%2C+J">Jieren Cheng</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Chen%2C+J">Jing Chen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY)

</div>
<p class="mathjax">The accuracy of predicting the Producer Price Index (PPI) plays an
indispensable role in government economic work. However, it is difficult to
forecast the PPI. In our research, we first propose an unprecedented hybrid
model based on fuzzy information granulation that integrates the GA-SVR and
ARIMA (Autoregressive Integrated Moving Average Model) models. The
fuzzy-information-granulation-based GA-SVR-ARIMA hybrid model is intended to
deal with the problem of imprecision in PPI estimation. The proposed model
adopts the fuzzy information-granulation algorithm to
pre-classification-process monthly training samples of the PPI, and produced
three different sequences of fuzzy information granules, whose Support Vector
Regression (SVR) machine forecast models were separately established for their
Genetic Algorithm (GA) optimization parameters. Finally, the residual errors of
the GA-SVR model were rectified through ARIMA modeling, and the PPI estimate
was reached. Research shows that the PPI value predicted by this hybrid model
is more accurate than that predicted by other models, including ARIMA, GRNN,
and GA-SVR, following several comparative experiments. Research also indicates
the precision and validation of the PPI prediction of the hybrid model and
demonstrates that the model has consistent ability to leverage the forecasting
advantage of GA-SVR in non-linear space and of ARIMA in linear space.
</p>
</div>
</dd>
<dt><a name="item14">[14]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12035" title="Abstract">arXiv:1903.12035</a> [<a href="https://arxiv.org/pdf/1903.12035" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12035" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Quick inference for log Gaussian Cox processes with non-stationary  underlying random fields
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Dvo%C5%99%C3%A1k%2C+J">Ji&#x159;&#xed; Dvo&#x159;&#xe1;k</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=M%C3%B8ller%2C+J">Jesper M&#xf8;ller</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Mrkvi%C4%8Dka%2C+T">Tom&#xe1;&#x161; Mrkvi&#x10d;ka</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Soubeyrand%2C+S">Samuel Soubeyrand</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>

</div>
<p class="mathjax">For point patterns observed in natura, spatial heterogeneity is more the rule
than the exception. In numerous applications, this can be mathematically
handled by the flexible class of log Gaussian Cox processes (LGCPs); in brief,
a LGCP is a Cox process driven by an underlying log Gaussian random field (log
GRF). This allows the representation of point aggregation, point vacuum and
intermediate situations, with more or less rapid transitions between these
different states depending on the properties of GRF. Very often, the covariance
function of the GRF is assumed to be stationary. In this article, we give two
examples where the sizes (that is, the number of points) and the spatial
extents of point clusters are allowed to vary in space. To tackle such
features, we propose parametric and semiparametric models of non-stationary
LGCPs where the non-stationarity is included in both the mean function and the
covariance function of the GRF. Thus, in contrast to most other work on
inhomogeneous LGCPs, second-order intensity-reweighted stationarity is not
satisfied and the usual two step procedure for parameter estimation based on
e.g. composite likelihood does not easily apply. Instead we propose a fast
three step procedure based on composite likelihood. We apply our modelling and
estimation framework to analyse datasets dealing with fish aggregation in a
reservoir and with dispersal of biological particles.
</p>
</div>
</dd>
<dt><a name="item15">[15]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12044" title="Abstract">arXiv:1903.12044</a> [<a href="https://arxiv.org/pdf/1903.12044" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.12044" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.12044" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Convergence rates for optimised adaptive importance samplers
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Akyildiz%2C+%C3%96+D">&#xd6;mer Deniz Akyildiz</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=M%C3%ADguez%2C+J">Joaqu&#xed;n M&#xed;guez</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Methodology (stat.ME); Machine Learning (stat.ML)

</div>
<p class="mathjax">Adaptive importance samplers are adaptive Monte Carlo algorithms to estimate
expectations with respect to some target distribution which adapt themselves to
obtain better estimators over iterations. Although it is straightforward to
show that they have the same theoretical guarantees with importance sampling
with respect to the sample size, their behaviour over the number of iterations
has been relatively left unexplored despite these adaptive algorithms aim at
improving the proposal quality over time. In this work, we explore an
adaptation strategy based on convex optimisation which leads to a class of
adaptive importance samplers, termed optimised adaptive importance samplers
(OAIS). These samplers rely on an adaptation idea based on minimizing the
$\chi^2$-divergence between an exponential family proposal and the target. The
analysed algorithms are closely related to the adaptive importance samplers
which minimise the variance of the weight function. We first prove
non-asymptotic error bounds for the mean squared errors (MSEs) of these
algorithms, which explicitly depend on the number of iterations and the number
of particles together. The non-asymptotic bounds derived in this paper imply
that when the target is from the exponential family, the $L_2$ errors of the
optimised samplers converge to the perfect Monte Carlo sampling error
$\mathcal{O}(1/\sqrt{N})$. We also show that when the target is not from the
exponential family, the asymptotic error rate is
$\mathcal{O}(\sqrt{\rho^\star/N})$ where $\rho^\star$ is the minimum
$\chi^2$-divergence between the target and an exponential family proposal.
</p>
</div>
</dd>
<dt><a name="item16">[16]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12067" title="Abstract">arXiv:1903.12067</a> [<a href="https://arxiv.org/pdf/1903.12067" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12067" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Buffered environmental contours
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Dahl%2C+K+R">Kristina Rognlien Dahl</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Huseby%2C+A+B">Arne Bang Huseby</a>
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Safety and Reliability - Safe Societies in a Changing World.
  Proceedings of ESREL 2018, June 17-21, 2018. Taylor &amp; Francis. ISBN
  9781351174657. 281. s 2285 - 2292
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
<p class="mathjax">The main idea of this paper is to use the notion of buffered failure
probability from probabilistic structural design, to introduce buffered
environmental contours. Classical environmental contours are used in structural
design in order to obtain upper bounds on the failure probabilities of a large
class of designs. The purpose of buffered failure probabilities is the same.
However, in constrast to classical environmental contours, this new concept
does not just take into account failure vs. functioning, but also to which
extent the system is failing. For example, this is relevant when considering
the risk of flooding: We are not just interested in knowing whether a river has
flooded. The damages caused by the flooding greatly depends on how much the
water has risen above the standard level.
</p>
</div>
</dd>
<dt><a name="item17">[17]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12077" title="Abstract">arXiv:1903.12077</a> [<a href="https://arxiv.org/pdf/1903.12077" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.12077" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.12077" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Time series models for realized covariance matrices based on the  matrix-F distribution
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Zhou%2C+J">Jiayuan Zhou</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Jiang%2C+F">Feiyu Jiang</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Zhu%2C+K">Ke Zhu</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Li%2C+W+K">Wai Keung Li</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>; Econometrics (econ.EM); Methodology (stat.ME)

</div>
<p class="mathjax">We propose a new Conditional BEKK matrix-F (CBF) model for the time-varying
realized covariance (RCOV) matrices. This CBF model is capable of capturing
heavy-tailed RCOV, which is an important stylized fact but could not be handled
adequately by the Wishart-based models. To further mimic the long memory
feature of the RCOV, a special CBF model with the conditional heterogeneous
autoregressive (HAR) structure is introduced. Moreover, we give a systematical
study on the probabilistic properties and statistical inferences of the CBF
model, including exploring its stationarity, establishing the asymptotics of
its maximum likelihood estimator, and giving some new inner-product-based tests
for its model checking. In order to handle a large dimensional RCOV matrix, we
construct two reduced CBF models --- the variance-target CBF model (for
moderate but fixed dimensional RCOV matrix) and the factor CBF model (for high
dimensional RCOV matrix). For both reduced models, the asymptotic theory of the
estimated parameters is derived. The importance of our entire methodology is
illustrated by simulation results and two real examples.
</p>
</div>
</dd>
<dt><a name="item18">[18]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12078" title="Abstract">arXiv:1903.12078</a> [<a href="https://arxiv.org/pdf/1903.12078" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12078" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Error Analysis for the Particle Filter: Methods and Theoretical Support
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Liu%2C+Z">Ziyu Liu</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wei%2C+S">Shihong Wei</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Spall%2C+J+C">James C. Spall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Applications (stat.AP)

</div>
<p class="mathjax">The particle filter is a popular Bayesian filtering algorithm for use in
cases where the state-space model is nonlinear and/or the random terms (initial
state or noises) are non-Gaussian distributed. We study the behavior of the
error in the particle filter algorithm as the number of particles gets large.
After a decomposition of the error into two terms, we show that the difference
between the estimator and the conditional mean is asymptotically normal when
the resampling is done at every step in the filtering process. Two
nonlinear/non-Gaussian examples are tested to verify this conclusion.
</p>
</div>
</dd>
<dt><a name="item19">[19]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12125" title="Abstract">arXiv:1903.12125</a> [<a href="https://arxiv.org/pdf/1903.12125" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12125" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Nearest-Neighbor Neural Networks for Geostatistics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wang%2C+H">Haoyu Wang</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Guan%2C+Y">Yawen Guan</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Reich%2C+B+J">Brian J Reich</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
<p class="mathjax">Kriging is the predominant method used for spatial prediction, but relies on
the assumption that predictions are linear combinations of the observations.
Kriging often also relies on additional assumptions such as normality and
stationarity. We propose a more flexible spatial prediction method based on the
Nearest-Neighbor Neural Network (4N) process that embeds deep learning into a
geostatistical model. We show that the 4N process is a valid stochastic process
and propose a series of new ways to construct features to be used as inputs to
the deep learning model based on neighboring information. Our model framework
outperforms some existing state-of-art geostatistical modelling methods for
simulated non-Gaussian data and is applied to a massive forestry dataset.
</p>
</div>
</dd>
</dl>
<h3>Cross-lists for Fri, 29 Mar 19</h3>
<dl>
<dt><a name="item20">[20]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11626" title="Abstract">arXiv:1903.11626</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11626" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11626" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Bridging Adversarial Robustness and Gradient Interpretability
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim%2C+B">Beomsu Kim</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Seo%2C+J">Junghoon Seo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jeon%2C+T">Taegyun Jeon</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at the 2019 ICLR Workshop on Safe Machine Learning: Specification, Robustness, and Assurance (SafeML 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Adversarial training is a training scheme designed to counter adversarial
attacks by augmenting the training dataset with adversarial examples.
Surprisingly, several studies have observed that loss gradients from
adversarially trained DNNs are visually more interpretable than those from
standard DNNs. Although this phenomenon is interesting, there are only few
works that have offered an explanation. In this paper, we attempted to bridge
this gap between adversarial robustness and gradient interpretability. To this
end, we identified that loss gradients from adversarially trained DNNs align
better with human perception because adversarial training restricts gradients
closer to the image manifold. We then demonstrated that adversarial training
causes loss gradients to be quantitatively meaningful. Finally, we showed that
under the adversarial training framework, there exists an empirical trade-off
between test accuracy and loss gradient interpretability and proposed two
potential approaches to resolving this trade-off.
</p>
</div>
</dd>
<dt><a name="item21">[21]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11673" title="Abstract">arXiv:1903.11673</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11673" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11673" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Adversarial Deep Learning in EEG Biometrics
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ozdenizci%2C+O">Ozan Ozdenizci</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+Y">Ye Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koike-Akino%2C+T">Toshiaki Koike-Akino</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Erdogmus%2C+D">Deniz Erdogmus</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication by IEEE Signal Processing Letters
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> IEEE Signal Processing Letters, 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Signal Processing (eess.SP); Machine Learning (stat.ML)

</div>
<p class="mathjax">Deep learning methods for person identification based on
electroencephalographic (EEG) brain activity encounters the problem of
exploiting the temporally correlated structures or recording session specific
variability within EEG. Furthermore, recent methods have mostly trained and
evaluated based on single session EEG data. We address this problem from an
invariant representation learning perspective. We propose an adversarial
inference approach to extend such deep learning models to learn
session-invariant person-discriminative representations that can provide
robustness in terms of longitudinal usability. Using adversarial learning
within a deep convolutional network, we empirically assess and show
improvements with our approach based on longitudinally collected EEG data for
person identification from half-second EEG epochs.
</p>
</div>
</dd>
<dt><a name="item22">[22]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11680" title="Abstract">arXiv:1903.11680</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11680" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11680" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Gradient Descent with Early Stopping is Provably Robust to Label Noise  for Overparameterized Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li%2C+M">Mingchen Li</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Soltanolkotabi%2C+M">Mahdi Soltanolkotabi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Oymak%2C+S">Samet Oymak</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Modern neural networks are typically trained in an over-parameterized regime
where the parameters of the model far exceed the size of the training data. Due
to over-parameterization these neural networks in principle have the capacity
to (over)fit any set of labels including pure noise. Despite this high fitting
capacity, somewhat paradoxically, neural network models trained via first-order
methods continue to predict well on yet unseen test data. In this paper we take
a step towards demystifying this phenomena. In particular we show that first
order methods such as gradient descent are provably robust to noise/corruption
on a constant fraction of the labels despite over-parametrization under a rich
dataset model. In particular: i) First, we show that in the first few
iterations where the updates are still in the vicinity of the initialization
these algorithms only fit to the correct labels essentially ignoring the noisy
labels. ii) Secondly, we prove that to start to overfit to the noisy labels
these algorithms must stray rather far from from the initial model which can
only occur after many more iterations. Together, these show that gradient
descent with early stopping is provably robust to label noise and shed light on
empirical robustness of deep networks as well as commonly adopted heuristics to
prevent overfitting.
</p>
</div>
</dd>
<dt><a name="item23">[23]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11703" title="Abstract">arXiv:1903.11703</a> (cross-list from eess.SP) [<a href="https://arxiv.org/pdf/1903.11703" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11703" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Recurrent Neural Networks For Accurate RSSI Indoor Localization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Hoang%2C+M+T">Minh Tu Hoang</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Yuen%2C+B">Brosnan Yuen</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Dong%2C+X">Xiaodai Dong</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Lu%2C+T">Tao Lu</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Westendorp%2C+R">Robert Westendorp</a>, 
<a href="https://arxiv.org/search/eess?searchtype=author&amp;query=Reddy%2C+K">Kishore Reddy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Received signal strength indicator (RSSI), WiFi indoor localization, recurrent neuron network (RNN), long shortterm memory (LSTM), fingerprint-based localization
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Signal Processing (eess.SP)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This paper proposes recurrent neuron networks (RNNs) for a fingerprinting
indoor localization using WiFi. Instead of locating user's position one at a
time as in the cases of conventional algorithms, our RNN solution aims at
trajectory positioning and takes into account the relation among the received
signal strength indicator (RSSI) measurements in a trajectory. Furthermore, a
weighted average filter is proposed for both input RSSI data and sequential
output locations to enhance the accuracy among the temporal fluctuations of
RSSI. The results using different types of RNN including vanilla RNN, long
short-term memory (LSTM), gated recurrent unit (GRU) and bidirectional LSTM
(BiLSTM) are presented. On-site experiments demonstrate that the proposed
structure achieves an average localization error of $0.75$ m with $80\%$ of the
errors under $1$ m, which outperforms the conventional KNN algorithms and
probabilistic algorithms by approximately $30\%$ under the same test
environment.
</p>
</div>
</dd>
<dt><a name="item24">[24]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11719" title="Abstract">arXiv:1903.11719</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11719" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11719" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Fairness in Algorithmic Decision Making: An Excursion Through the Lens  of Causality
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khademi%2C+A">Aria Khademi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee%2C+S">Sanghack Lee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Foley%2C+D">David Foley</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Honavar%2C+V">Vasant Honavar</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 7 pages, 2 figures, 2 tables.To appear in Proceedings of the International Conference on World Wide Web (WWW), 2019
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">As virtually all aspects of our lives are increasingly impacted by
algorithmic decision making systems, it is incumbent upon us as a society to
ensure such systems do not become instruments of unfair discrimination on the
basis of gender, race, ethnicity, religion, etc. We consider the problem of
determining whether the decisions made by such systems are discriminatory,
through the lens of causal models. We introduce two definitions of group
fairness grounded in causality: fair on average causal effect (FACE), and fair
on average causal effect on the treated (FACT). We use the Rubin-Neyman
potential outcomes framework for the analysis of cause-effect relationships to
robustly estimate FACE and FACT. We demonstrate the effectiveness of our
proposed approach on synthetic data. Our analyses of two real-world data sets,
the Adult income data set from the UCI repository (with gender as the protected
attribute), and the NYC Stop and Frisk data set (with race as the protected
attribute), show that the evidence of discrimination obtained by FACE and FACT,
or lack thereof, is often in agreement with the findings from other studies. We
further show that FACT, being somewhat more nuanced compared to FACE, can yield
findings of discrimination that differ from those obtained using FACE.
</p>
</div>
</dd>
<dt><a name="item25">[25]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11774" title="Abstract">arXiv:1903.11774</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11774" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.11774" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.11774" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> How to pick the domain randomization parameters for sim-to-real transfer  of reinforcement learning policies?
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vuong%2C+Q">Quan Vuong</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vikram%2C+S">Sharad Vikram</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su%2C+H">Hao Su</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao%2C+S">Sicun Gao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Christensen%2C+H+I">Henrik I. Christensen</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 2-page extended abstract
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Machine Learning (stat.ML)

</div>
<p class="mathjax">Recently, reinforcement learning (RL) algorithms have demonstrated remarkable
success in learning complicated behaviors from minimally processed input.
However, most of this success is limited to simulation. While there are
promising successes in applying RL algorithms directly on real systems, their
performance on more complex systems remains bottle-necked by the relative data
inefficiency of RL algorithms. Domain randomization is a promising direction of
research that has demonstrated impressive results using RL algorithms to
control real robots. At a high level, domain randomization works by training a
policy on a distribution of environmental conditions in simulation. If the
environments are diverse enough, then the policy trained on this distribution
will plausibly generalize to the real world. A human-specified design choice in
domain randomization is the form and parameters of the distribution of
simulated environments. It is unclear how to the best pick the form and
parameters of this distribution and prior work uses hand-tuned distributions.
This extended abstract demonstrates that the choice of the distribution plays a
major role in the performance of the trained policies in the real world and
that the parameter of this distribution can be optimized to maximize the
performance of the trained policies in the real world
</p>
</div>
</dd>
<dt><a name="item26">[26]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11775" title="Abstract">arXiv:1903.11775</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11775" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Atrial Fibrillation Detection Using Deep Features and Convolutional  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ross-Howe%2C+S">Sara Ross-Howe</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tizhoosh%2C+H+R">H.R. Tizhoosh</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted for publication in the IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Atrial fibrillation is a cardiac arrhythmia that affects an estimated 33.5
million people globally and is the potential cause of 1 in 3 strokes in people
over the age of 60. Detection and diagnosis of atrial fibrillation (AFIB) is
done noninvasively in the clinical environment through the evaluation of
electrocardiograms (ECGs). Early research into automated methods for the
detection of AFIB in ECG signals focused on traditional bio-medical signal
analysis to extract important features for use in statistical classification
models. Artificial intelligence models have more recently been used that employ
convolutional and/or recurrent network architectures. In this work, significant
time and frequency domain characteristics of the ECG signal are extracted by
applying the short-time Fourier trans-form and then visually representing the
information in a spectrogram. Two different classification approaches were
investigated that utilized deep features in the spectrograms construct-ed from
ECG segments. The first approach used a pretrained DenseNet model to extract
features that were then classified using Support Vector Machines, and the
second approach used the spectrograms as direct input into a convolutional
network. Both approaches were evaluated against the MIT-BIH AFIB dataset, where
the convolutional network approach achieved a classification accuracy of
93.16%. While these results do not surpass established automated atrial
fibrillation detection methods, they are promising and warrant further
investigation given they did not require any noise prefiltering, hand-crafted
features, nor a reliance on beat detection.
</p>
</div>
</dd>
<dt><a name="item27">[27]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11780" title="Abstract">arXiv:1903.11780</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11780" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11780" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Wasserstein Dependency Measure for Representation Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ozair%2C+S">Sherjil Ozair</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lynch%2C+C">Corey Lynch</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bengio%2C+Y">Yoshua Bengio</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=van+den+Oord%2C+A">Aaron van den Oord</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sermanet%2C+P">Pierre Sermanet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Mutual information maximization has emerged as a powerful learning objective
for unsupervised representation learning obtaining state-of-the-art performance
in applications such as object recognition, speech recognition, and
reinforcement learning. However, such approaches are fundamentally limited
since a tight lower bound of mutual information requires sample size
exponential in the mutual information. This limits the applicability of these
approaches for prediction tasks with high mutual information, such as in video
understanding or reinforcement learning. In these settings, such techniques are
prone to overfit, both in theory and in practice, and capture only a few of the
relevant factors of variation. This leads to incomplete representations that
are not optimal for downstream tasks. In this work, we empirically demonstrate
that mutual information-based representation learning approaches do fail to
learn complete representations on a number of designed and real-world tasks. To
mitigate these problems we introduce the Wasserstein dependency measure, which
learns more complete representations by using the Wasserstein distance instead
of the KL divergence in the mutual information estimator. We show that a
practical approximation to this theoretically motivated solution, constructed
using Lipschitz constraint techniques from the GAN literature, achieves
substantially improved results on tasks where incomplete representations are a
major challenge.
</p>
</div>
</dd>
<dt><a name="item28">[28]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11789" title="Abstract">arXiv:1903.11789</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11789" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11789" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Step Change Improvement in ADMET Prediction with PotentialNet Deep  Featurization
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feinberg%2C+E+N">Evan N. Feinberg</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sheridan%2C+R">Robert Sheridan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Joshi%2C+E">Elizabeth Joshi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pande%2C+V+S">Vijay S. Pande</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng%2C+A+C">Alan C. Cheng</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 41 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">The Absorption, Distribution, Metabolism, Elimination, and Toxicity (ADMET)
properties of drug candidates are estimated to account for up to 50% of all
clinical trial failures. Predicting ADMET properties has therefore been of
great interest to the cheminformatics and medicinal chemistry communities in
recent decades. Traditional cheminformatics approaches, whether the learner is
a random forest or a deep neural network, leverage fixed fingerprint feature
representations of molecules. In contrast, in this paper, we learn the features
most relevant to each chemical task at hand by representing each molecule
explicitly as a graph, where each node is an atom and each edge is a bond. By
applying graph convolutions to this explicit molecular representation, we
achieve, to our knowledge, unprecedented accuracy in prediction of ADMET
properties. By challenging our methodology with rigorous cross-validation
procedures and prospective analyses, we show that deep featurization better
enables molecular predictors to not only interpolate but also extrapolate to
new regions of chemical space.
</p>
</div>
</dd>
<dt><a name="item29">[29]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11797" title="Abstract">arXiv:1903.11797</a> (cross-list from q-bio.PE) [<a href="https://arxiv.org/pdf/1903.11797" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11797" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Estimating effective population size changes from preferentially sampled  genetic sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Karcher%2C+M+D">Michael D. Karcher</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Suchard%2C+M+A">Marc A. Suchard</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Dudas%2C+G">Gytis Dudas</a>, 
<a href="https://arxiv.org/search/q-bio?searchtype=author&amp;query=Minin%2C+V+N">Vladimir N. Minin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 47 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Populations and Evolution (q-bio.PE)</span>; Methodology (stat.ME)

</div>
<p class="mathjax">Coalescent theory combined with statistical modeling allows us to estimate
effective population size fluctuations from molecular sequences of individuals
sampled from a population of interest. When sequences are sampled serially
through time and the distribution of the sampling times depends on the
effective population size, explicit statistical modeling of sampling times
improves population size estimation. Previous work assumed that the genealogy
relating sampled sequences is known and modeled sampling times as an
inhomogeneous Poisson process with log-intensity equal to a linear function of
the log-transformed effective population size. We improve this approach in two
ways. First, we extend the method to allow for joint Bayesian estimation of the
genealogy, effective population size trajectory, and other model parameters.
Next, we improve the sampling time model by incorporating additional sources of
information in the form of time-varying covariates. We validate our new
modeling framework using a simulation study and apply our new methodology to
analyses of population dynamics of seasonal influenza and to the recent Ebola
virus outbreak in West Africa.
</p>
</div>
</dd>
<dt><a name="item30">[30]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11835" title="Abstract">arXiv:1903.11835</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11835" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11835" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Survey on Graph Kernels
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kriege%2C+N+M">Nils M. Kriege</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Johansson%2C+F+D">Fredrik D. Johansson</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Morris%2C+C">Christopher Morris</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph kernels have become an established and widely-used technique for
solving classification tasks on graphs. This survey gives a comprehensive
overview of techniques for kernel-based graph classification developed in the
past 15 years. We describe and categorize graph kernels based on properties
inherent to their design, such as the nature of their extracted graph features,
their method of computation and their applicability to problems in practice. In
an extensive experimental evaluation, we study the classification accuracy of a
large suite of graph kernels on established benchmarks as well as new datasets.
We compare the performance of popular kernels with several baseline methods and
study the effect of applying a Gaussian RBF kernel to the metric induced by a
graph kernel. In doing so, we find that simple baselines become competitive
after this transformation on some datasets. Moreover, we study the extent to
which existing graph kernels agree in their predictions (and prediction errors)
and obtain a data-driven categorization of kernels as result. Finally, based on
our experimental results, we derive a practitioner's guide to kernel-based
graph classification.
</p>
</div>
</dd>
<dt><a name="item31">[31]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11900" title="Abstract">arXiv:1903.11900</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11900" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11900" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Model Vulnerability to Distributional Shifts over Image Transformation  Sets
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Volpi%2C+R">Riccardo Volpi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Murino%2C+V">Vittorio Murino</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">We are concerned with the vulnerability of computer vision models to
distributional shifts. We cast this problem in terms of combinatorial
optimization, evaluating the regions in the input space where a (black-box)
model is more vulnerable. This is carried out by combining image
transformations from a given set and standard search algorithms. We embed this
idea in a training procedure, where we define new data augmentation rules over
iterations, accordingly to the image transformations that the current model is
most vulnerable to. An empirical evaluation on classification and semantic
segmentation problems suggests that the devised algorithm allows to train
models more robust against content-preserving image transformations, and in
general, against distributional shifts.
</p>
</div>
</dd>
<dt><a name="item32">[32]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11960" title="Abstract">arXiv:1903.11960</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11960" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11960" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning Discrete Structures for Graph Neural Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Franceschi%2C+L">Luca Franceschi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Niepert%2C+M">Mathias Niepert</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pontil%2C+M">Massimiliano Pontil</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He%2C+X">Xiao He</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 18 pages
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Graph neural networks (GNNs) are a popular class of machine learning models
whose major advantage is their ability to incorporate a sparse and discrete
dependency structure between data points. Unfortunately, GNNs can only be used
when such a graph-structure is available. In practice, however, real-world
graphs are often noisy and incomplete or might not be available at all. With
this work, we propose to jointly learn the graph structure and the parameters
of graph convolutional networks (GCNs) by approximately solving a bilevel
program that learns a discrete probability distribution on the edges of the
graph. This allows one to apply GCNs not only in scenarios where the given
graph is incomplete or corrupted but also in those where a graph is not
available. We conduct a series of experiments that analyze the behavior of the
proposed method and demonstrate that it outperforms related methods by a
significant margin.
</p>
</div>
</dd>
<dt><a name="item33">[33]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11981" title="Abstract">arXiv:1903.11981</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11981" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11981" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Regularizing Trajectory Optimization with Denoising Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Boney%2C+R">Rinu Boney</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Di+Palo%2C+N">Norman Di Palo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Berglund%2C+M">Mathias Berglund</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ilin%2C+A">Alexander Ilin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kannala%2C+J">Juho Kannala</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rasmus%2C+A">Antti Rasmus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Valpola%2C+H">Harri Valpola</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Robotics (cs.RO); Machine Learning (stat.ML)

</div>
<p class="mathjax">Trajectory optimization with learned dynamics models can often suffer from
erroneous predictions of out-of-distribution trajectories. We propose to
regularize trajectory optimization by means of a denoising autoencoder that is
trained on the same trajectories as the dynamics model. We visually demonstrate
the effectiveness of the regularization in gradient-based trajectory
optimization for open-loop control of an industrial process. We compare with
recent model-based reinforcement learning algorithms on a set of popular motor
control tasks to demonstrate that the denoising regularization enables
state-of-the-art sample-efficiency. We demonstrate the efficacy of the proposed
method in regularizing both gradient-based and gradient-free trajectory
optimization.
</p>
</div>
</dd>
<dt><a name="item34">[34]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11983" title="Abstract">arXiv:1903.11983</a> (cross-list from cs.IR) [<a href="https://arxiv.org/pdf/1903.11983" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Sentiment Analysis on IMDB Movie Comments and Twitter Data by Machine  Learning and Vector Space Techniques
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tar%C4%B1mer%2C+%C4%B0">&#x130;lhan Tar&#x131;mer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=%C3%87oban%2C+A">Adil &#xc7;oban</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kocaman%2C+A+E">Arif Emre Kocaman</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, submitted to CIEA2018 (<a href="http://iciea.cumhuriyet.edu.tr/">this http URL</a>)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Information Retrieval (cs.IR)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This study's goal is to create a model of sentiment analysis on a 2000 rows
IMDB movie comments and 3200 Twitter data by using machine learning and vector
space techniques; positive or negative preliminary information about the text
is to provide. In the study, a vector space was created in the KNIME Analytics
platform, and a classification study was performed on this vector space by
Decision Trees, Na\"ive Bayes and Support Vector Machines classification
algorithms. The conclusions obtained were compared in terms of each algorithms.
The classification results for IMDB movie comments are obtained as 94,00%,
73,20%, and 85,50% by Decision Tree, Naive Bayes and SVM algorithms. The
classification results for Twitter data set are presented as 82,76%, 75,44% and
72,50% by Decision Tree, Naive Bayes SVM algorithms as well. It is seen that
the best classification results presented in both data sets are which
calculated by SVM algorithm.
</p>
</div>
</dd>
<dt><a name="item35">[35]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11991" title="Abstract">arXiv:1903.11991</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.11991" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> PAL: A fast DNN optimization method based on curvature information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mutschler%2C+M">Maximus Mutschler</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zell%2C+A">Andreas Zell</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">We present a novel optimizer for deep neural networks that combines the ideas
of Netwon's method and line search to efficiently compute and utilize curvature
information. Our work is based on empirical observation suggesting that the
loss function can be approximated by a parabola in negative gradient direction.
Due to this approximation, we are able to perform a variable and loss function
dependent parameter update by jumping directly into the minimum of the
approximated parabola. To evaluate our optimizer, we performed multiple
comprehensive hyperparameter grid searches for which we trained more than 20000
networks in total. We can show that PAL outperforms RMSPROP, and can outperform
gradient descent with momentum and ADAM on large-scale high-dimensional machine
learning problems. Furthermore, PAL requires up to 52.2% less training epochs.
PyTorch and TensorFlow implementations are provided at
https://github.com/cogsys-tuebingen/PAL.
</p>
</div>
</dd>
<dt><a name="item36">[36]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12019" title="Abstract">arXiv:1903.12019</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.12019" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1903.12019" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1903.12019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multimodal Deep Network Embedding with Integrated Structure and  Attribute Information
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng%2C+C">Conghui Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan%2C+L">Li Pan</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu%2C+P">Peng Wu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 15 pages, 10 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
<p class="mathjax">Network embedding is the process of learning low-dimensional representations
for nodes in a network, while preserving node features. Existing studies only
leverage network structure information and focus on preserving structural
features. However, nodes in real-world networks often have a rich set of
attributes providing extra semantic information. It has been demonstrated that
both structural and attribute features are important for network analysis
tasks. To preserve both features, we investigate the problem of integrating
structure and attribute information to perform network embedding and propose a
Multimodal Deep Network Embedding (MDNE) method. MDNE captures the non-linear
network structures and the complex interactions among structures and
attributes, using a deep model consisting of multiple layers of non-linear
functions. Since structures and attributes are two different types of
information, a multimodal learning method is adopted to pre-process them and
help the model to better capture the correlations between node structure and
attribute information. We employ both structural proximity and attribute
proximity in the loss function to preserve the respective features and the
representations are obtained by minimizing the loss function. Results of
extensive experiments on four real-world datasets show that the proposed method
performs significantly better than baselines on a variety of tasks, which
demonstrate the effectiveness and generality of our method.
</p>
</div>
</dd>
<dt><a name="item37">[37]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12069" title="Abstract">arXiv:1903.12069</a> (cross-list from cs.CY) [<a href="https://arxiv.org/pdf/1903.12069" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The Virtual Doctor: An Interactive Artificial Intelligence based on Deep  Learning for Non-Invasive Prediction of Diabetes
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sp%C3%A4nig%2C+S">Sebastian Sp&#xe4;nig</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Emberger-Klein%2C+A">Agnes Emberger-Klein</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sowa%2C+J">Jan-Peter Sowa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Canbay%2C+A">Ali Canbay</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Menrad%2C+K">Klaus Menrad</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Heider%2C+D">Dominik Heider</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 16 pages, 4 figues
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Artificial intelligence (AI) will pave the way to a new era in medicine.
However, currently available AI systems do not interact with a patient, e.g.,
for anamnesis, and thus are only used by the physicians for predictions in
diagnosis or prognosis. However, these systems are widely used, e.g., in
diabetes or cancer prediction. In the current study, we developed an AI that is
able to interact with a patient (virtual doctor) by using a speech recognition
and speech synthesis system and thus can autonomously interact with the
patient, which is particularly important for, e.g., rural areas, where the
availability of primary medical care is strongly limited by low population
densities. As a proof-of-concept, the system is able to predict type 2 diabetes
mellitus (T2DM) based on non-invasive sensors and deep neural networks.
Moreover, the system provides an easy-to-interpret probability estimation for
T2DM for a given patient. Besides the development of the AI, we further
analyzed the acceptance of young people for AI in healthcare to estimate the
impact of such system in the future.
</p>
</div>
</dd>
<dt><a name="item38">[38]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12070" title="Abstract">arXiv:1903.12070</a> (cross-list from cs.CY) [<a href="https://arxiv.org/pdf/1903.12070" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Comprehensive Analysis of Dynamic Message Sign Impact on Driver  Behavior: A Random Forest Approach
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Banerjee%2C+S">Snehanshu Banerjee</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jeihani%2C+M">Mansoureh Jeihani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brown%2C+D+D">Danny D. Brown</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ahangari%2C+S">Samira Ahangari</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 12 pages, 12 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">This study investigates the potential effects of different Dynamic Message
Signs (DMSs) on driver behavior using a full-scale high-fidelity driving
simulator. Different DMSs are categorized by their content, structure, and type
of messages. A random forest algorithm is used for three separate behavioral
analyses; a route diversion analysis, a route choice analysis and a compliance
analysis; to identify the potential and relative influences of different DMSs
on these aspects of driver behavior. A total of 390 simulation runs are
conducted using a sample of 65 participants from diverse socioeconomic
backgrounds. Results obtained suggest that DMSs displaying lane closure and
delay information with advisory messages are most influential with regards to
diversion while color-coded DMSs and DMSs with avoid route advice are the top
contributors impacting route choice decisions and DMS compliance. In this
first-of-a-kind study, based on the responses to the pre and post simulation
surveys as well as results obtained from the analysis of
driving-simulation-session data, the authors found that color-blind-friendly,
color-coded DMSs are more effective than alphanumeric DMSs - especially in
scenarios that demand high compliance from drivers. The increased effectiveness
may be attributed to reduced comprehension time and ease with which such DMSs
are understood by a greater percentage of road users.
</p>
</div>
</dd>
<dt><a name="item39">[39]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12074" title="Abstract">arXiv:1903.12074</a> (cross-list from cs.CY) [<a href="https://arxiv.org/pdf/1903.12074" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12074" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Interpretation of machine learning predictions for patient outcomes in  electronic health records
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=La+Cava%2C+W">William La Cava</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bauer%2C+C">Christopher Bauer</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Moore%2C+J+H">Jason H. Moore</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pendergrass%2C+S+A">Sarah A Pendergrass</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 5 figures, submitted to AMIA Symposium
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">Electronic health records are an increasingly important resource for
understanding the interactions between patient health, environment, and
clinical decisions. In this paper we report an empirical study of predictive
modeling of several patient outcomes using three state-of-the-art machine
learning methods. Our primary goal is to validate the models by interpreting
the importance of predictors in the final models. Central to interpretation is
the use of feature importance scores, which vary depending on the underlying
methodology. In order to assess feature importance, we compared univariate
statistical tests, information-theoretic measures, permutation testing, and
normalized coefficients from multivariate logistic regression models. In
general we found poor correlation between methods in their assessment of
feature importance, even when their performance is comparable and relatively
good. However, permutation tests applied to random forest and gradient boosting
models showed the most agreement, and the importance scores matched the
clinical interpretation most frequently.
</p>
</div>
</dd>
<dt><a name="item40">[40]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12080" title="Abstract">arXiv:1903.12080</a> (cross-list from cs.CY) [<a href="https://arxiv.org/pdf/1903.12080" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Detecting Activities of Daily Living and Routine Behaviours in Dementia  Patients Living Alone Using Smart Meter Load Disaggregation
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chalmers%2C+C">C. Chalmers</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fergus%2C+P">P.Fergus</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Montanez%2C+C+A+C">C. Aday Curbelo Montanez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sikdar%2C+S">S.Sikdar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ball%2C+F">F.Ball</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kendall%2C+B">B. Kendall</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computers and Society (cs.CY)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
<p class="mathjax">The emergence of an ageing population is a significant public health concern.
This has led to an increase in the number of people living with progressive
neurodegenerative disorders like dementia. Consequently, the strain this is
places on health and social care services means providing 24-hour monitoring is
not sustainable. Technological intervention is being considered, however no
solution exists to non-intrusively monitor the independent living needs of
patients with dementia. As a result many patients hit crisis point before
intervention and support is provided. In parallel, patient care relies on
feedback from informal carers about significant behavioural changes. Yet, not
all people have a social support network and early intervention in dementia
care is often missed. The smart meter rollout has the potential to change this.
Using machine learning and signal processing techniques, a home energy supply
can be disaggregated to detect which home appliances are turned on and off.
This will allow Activities of Daily Living (ADLs) to be assessed, such as
eating and drinking, and observed changes in routine to be detected for early
intervention. The primary aim is to help reduce deterioration and enable
patients to stay in their homes for longer. A Support Vector Machine (SVM) and
Random Decision Forest classifier are modelled using data from three test
homes. The trained models are then used to monitor two patients with dementia
during a six-month clinical trial undertaken in partnership with Mersey Care
NHS Foundation Trust. In the case of load disaggregation for appliance
detection, the SVM achieved (AUC=0.86074, Sen=0.756 and Spec=0.92838). While
the Decision Forest achieved (AUC=0.9429, Sen=0.9634 and Spec=0.9634). ADLs are
also analysed to identify the behavioural patterns of the occupant while
detecting alterations in routine.
</p>
</div>
</dd>
<dt><a name="item41">[41]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12090" title="Abstract">arXiv:1903.12090</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.12090" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12090" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Learning to Weight for Text Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fern%C3%A1ndez%2C+A+M">Alejandro Moreo Fern&#xe1;ndez</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Esuli%2C+A">Andrea Esuli</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sebastiani%2C+F">Fabrizio Sebastiani</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear in IEEE Transactions on Knowledge and Data Engineering
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Information Retrieval (cs.IR); Machine Learning (stat.ML)

</div>
<p class="mathjax">In information retrieval (IR) and related tasks, term weighting approaches
typically consider the frequency of the term in the document and in the
collection in order to compute a score reflecting the importance of the term
for the document. In tasks characterized by the presence of training data (such
as text classification) it seems logical that the term weighting function
should take into account the distribution (as estimated from training data) of
the term across the classes of interest. Although `supervised term weighting'
approaches that use this intuition have been described before, they have failed
to show consistent improvements. In this article we analyse the possible
reasons for this failure, and call consolidated assumptions into question.
Following this criticism we propose a novel supervised term weighting approach
that, instead of relying on any predefined formula, learns a term weighting
function optimised on the training set of interest; we dub this approach
\emph{Learning to Weight} (LTW). The experiments that we run on several
well-known benchmarks, and using different learning methods, show that our
method outperforms previous term weighting approaches in text classification.
</p>
</div>
</dd>
<dt><a name="item42">[42]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12094" title="Abstract">arXiv:1903.12094</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.12094" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Barking up the Right Tree: Improving Cross-Corpus Speech Emotion  Recognition with Adversarial Discriminative Domain Generalization (ADDoG)
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gideon%2C+J">John Gideon</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McInnis%2C+M+G">Melvin G McInnis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Provost%2C+E+M">Emily Mower Provost</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS); Machine Learning (stat.ML)

</div>
<p class="mathjax">Automatic speech emotion recognition provides computers with critical context
to enable user understanding. While methods trained and tested within the same
dataset have been shown successful, they often fail when applied to unseen
datasets. To address this, recent work has focused on adversarial methods to
find more generalized representations of emotional speech. However, many of
these methods have issues converging, and only involve datasets collected in
laboratory conditions. In this paper, we introduce Adversarial Discriminative
Domain Generalization (ADDoG), which follows an easier to train "meet in the
middle" approach. The model iteratively moves representations learned for each
dataset closer to one another, improving cross-dataset generalization. We also
introduce Multiclass ADDoG, or MADDoG, which is able to extend the proposed
method to more than two datasets, simultaneously. Our results show consistent
convergence for the introduced methods, with significantly improved results
when not using labels from the target dataset. We also show how, in most cases,
ADDoG and MADDoG can be used to improve upon baseline state-of-the-art methods
when target dataset labels are added and in-the-wild data are considered. Even
though our experiments focus on cross-corpus speech emotion, these methods
could be used to remove unwanted factors of variation in other settings.
</p>
</div>
</dd>
<dt><a name="item43">[43]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12127" title="Abstract">arXiv:1903.12127</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.12127" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12127" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Latent Class Analysis to Identify ARDS Sub-phenotypes for Enhanced  Machine Learning Predictive Performance
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+T">Tony Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tschampel%2C+T">Tim Tschampel</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Apostolova%2C+E">Emilia Apostolova</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Velez%2C+T">Tom Velez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Work in progress, preliminary results
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Applications (stat.AP); Machine Learning (stat.ML)

</div>
<p class="mathjax">In this work, we utilize Machine Learning for early recognition of patients
at high risk of acute respiratory distress syndrome (ARDS), which is critical
for successful prevention strategies for this devastating syndrome. The
difficulty in early ARDS recognition stems from its complex and heterogenous
nature. In this study, we integrate knowledge of the heterogeneity of ARDS
patients into predictive model building. Using MIMIC-III data, we first apply
latent class analysis (LCA) to identify homogeneous sub-groups in the ARDS
population, and then build predictive models on the partitioned data. The
results indicate that significantly improved performances of prediction can be
obtained for two of the three identified sub-phenotypes of ARDS. Experiments
suggests that identifying sub-phenotypes is beneficial for building predictive
model for ARDS.
</p>
</div>
</dd>
<dt><a name="item44">[44]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12128" title="Abstract">arXiv:1903.12128</a> (cross-list from physics.comp-ph) [<a href="https://arxiv.org/pdf/1903.12128" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12128" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Using Gaussian process regression for efficient parameter reconstruction
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Schneider%2C+P">Philipp-Immanuel Schneider</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Hammerschmidt%2C+M">Martin Hammerschmidt</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Zschiedrich%2C+L">Lin Zschiedrich</a>, 
<a href="https://arxiv.org/search/physics?searchtype=author&amp;query=Burger%2C+S">Sven Burger</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures
</div>
<div class="list-journal-ref">
<span class="descriptor">Journal-ref:</span> Proc. SPIE 10959, Metrology, Inspection, and Process Control for
  Microlithography XXXIII, 1095911 (26 March 2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computational Physics (physics.comp-ph)</span>; Data Analysis, Statistics and Probability (physics.data-an); Machine Learning (stat.ML)

</div>
<p class="mathjax">Optical scatterometry is a method to measure the size and shape of periodic
micro- or nanostructures on surfaces. For this purpose the geometry parameters
of the structures are obtained by reproducing experimental measurement results
through numerical simulations. We compare the performance of Bayesian
optimization to different local minimization algorithms for this numerical
optimization problem. Bayesian optimization uses Gaussian-process regression to
find promising parameter values. We examine how pre-computed simulation results
can be used to train the Gaussian process and to accelerate the optimization.
</p>
</div>
</dd>
<dt><a name="item45">[45]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12141" title="Abstract">arXiv:1903.12141</a> (cross-list from cs.LG) [<a href="https://arxiv.org/pdf/1903.12141" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12141" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Improving MAE against CCE under Label Noise
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang%2C+X">Xinshao Wang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kodirov%2C+E">Elyor Kodirov</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hua%2C+Y">Yang Hua</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Robertson%2C+N+M">Neil M. Robertson</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> IMAE is a good alternative for CCE. Overfitting, Underfitting, Noise Robustness, Label Noise
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
<p class="mathjax">Label noise is inherent in many deep learning tasks when the training set
becomes large. A typical approach to tackle noisy labels is using robust loss
functions. Categorical cross entropy (CCE) is a successful loss function in
many applications. However, CCE is also notorious for fitting samples with
corrupted labels easily. In contrast, mean absolute error (MAE) is
noise-tolerant theoretically, but it generally works much worse than CCE in
practice. In this work, we have three main points. First, to explain why MAE
generally performs much worse than CCE, we introduce a new understanding of
them fundamentally by exposing their intrinsic sample weighting schemes from
the perspective of every sample's gradient magnitude with respect to logit
vector. Consequently, we find that MAE's differentiation degree over training
examples is too small so that informative ones cannot contribute enough against
the non-informative during training. Therefore, MAE generally underfits
training data when noise rate is high. Second, based on our finding, we propose
an improved MAE (IMAE), which inherits MAE's good noise-robustness. Moreover,
the differentiation degree over training data points is controllable so that
IMAE addresses the underfitting problem of MAE. Third, the effectiveness of
IMAE against CCE and MAE is evaluated empirically with extensive experiments,
which focus on image classification under synthetic corrupted labels and video
retrieval under real noisy labels.
</p>
</div>
</dd>
<dt><a name="item46">[46]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.12173" title="Abstract">arXiv:1903.12173</a> (cross-list from astro-ph.CO) [<a href="https://arxiv.org/pdf/1903.12173" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.12173" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Painting with baryons: augmenting N-body simulations with gas using deep  generative models
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&amp;query=Tr%C3%B6ster%2C+T">Tilman Tr&#xf6;ster</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&amp;query=Ferguson%2C+C">Cameron Ferguson</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&amp;query=Harnois-D%C3%A9raps%2C+J">Joachim Harnois-D&#xe9;raps</a>, 
<a href="https://arxiv.org/search/astro-ph?searchtype=author&amp;query=McCarthy%2C+I+G">Ian G. McCarthy</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Comments welcome. Submitted to MNRAS Letters. Code and trained models can be found at <a href="https://www.github.com/tilmantroester/baryon_painter">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Cosmology and Nongalactic Astrophysics (astro-ph.CO)</span>; Instrumentation and Methods for Astrophysics (astro-ph.IM); Machine Learning (stat.ML)

</div>
<p class="mathjax">Running hydrodynamical simulations to produce mock data of large-scale
structure and baryonic probes, such as the thermal Sunyaev-Zeldovich (tSZ)
effect, at cosmological scales is computationally challenging. We propose to
leverage the expressive power of deep generative models to find an effective
description of the large-scale gas distribution and temperature. We train two
deep generative models, a variational auto-encoder and a generative adversarial
network, on pairs of matter density and pressure slices from the BAHAMAS
hydrodynamical simulation. The trained models are able to successfully map
matter density to the corresponding gas pressure. We then apply the trained
models on 100 lines-of-sight from SLICS, a suite of N-body simulations
optimised for weak lensing covariance estimation, to generate maps of the tSZ
effect. The generated tSZ maps are found to be statistically consistent with
those from BAHAMAS. We conclude by considering a specific observable, the
angular cross-power spectrum between the weak lensing convergence and the tSZ
effect and its variance, where we find excellent agreement between the
predictions from BAHAMAS and SLICS, thus enabling the use of SLICS for tSZ
covariance estimation.
</p>
</div>
</dd>
</dl>
<h3>Replacements for Fri, 29 Mar 19</h3>
<dl>
<dt><a name="item47">[47]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1707.09714" title="Abstract">arXiv:1707.09714</a> (replaced) [<a href="https://arxiv.org/pdf/1707.09714" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1707.09714" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Geometric Variational Approach to Bayesian Inference
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Saha%2C+A">Abhijoy Saha</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Bharath%2C+K">Karthik Bharath</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kurtek%2C+S">Sebastian Kurtek</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
</div>
</dd>
<dt><a name="item48">[48]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1710.10919" title="Abstract">arXiv:1710.10919</a> (replaced) [<a href="https://arxiv.org/pdf/1710.10919" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1710.10919" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Kernel-Based Methods for Non-Linear Reduced Modeling
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=H%C3%A9as%2C+P">Patrick H&#xe9;as</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Herzet%2C+C">C&#xe9;dric Herzet</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>

</div>
</div>
</dd>
<dt><a name="item49">[49]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1805.01862" title="Abstract">arXiv:1805.01862</a> (replaced) [<a href="https://arxiv.org/pdf/1805.01862" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1805.01862" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lasso, knockoff and Gaussian covariates: a comparison
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Davies%2C+L">Laurie Davies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 79 pages Corrects some errors in the first version and includes files to enable the reader to reproduce the comparison
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
</div>
</dd>
<dt><a name="item50">[50]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1805.02988" title="Abstract">arXiv:1805.02988</a> (replaced) [<a href="https://arxiv.org/pdf/1805.02988" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1805.02988" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Hierarchical inference for genome-wide association studies: a view on  methodology with software
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Renaux%2C+C">Claude Renaux</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Buzdugan%2C+L">Laura Buzdugan</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kalisch%2C+M">Markus Kalisch</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=B%C3%BChlmann%2C+P">Peter B&#xfc;hlmann</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
<dt><a name="item51">[51]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1806.09386" title="Abstract">arXiv:1806.09386</a> (replaced) [<a href="https://arxiv.org/pdf/1806.09386" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1806.09386" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Treatment effects beyond the mean using GAMLSS
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Hohberg%2C+M">Maike Hohberg</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=P%C3%BCtz%2C+P">Peter P&#xfc;tz</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Kneib%2C+T">Thomas Kneib</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
<dt><a name="item52">[52]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1806.10008" title="Abstract">arXiv:1806.10008</a> (replaced) [<a href="https://arxiv.org/pdf/1806.10008" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1806.10008" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The conditionality principle in high-dimensional regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Azriel%2C+D">David Azriel</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>

</div>
</div>
</dd>
<dt><a name="item53">[53]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1807.07978" title="Abstract">arXiv:1807.07978</a> (replaced) [<a href="https://arxiv.org/pdf/1807.07978" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1807.07978" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Ilyas%2C+A">Andrew Ilyas</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Engstrom%2C+L">Logan Engstrom</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Madry%2C+A">Aleksander Madry</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> To appear at ICLR 2019; Code available at <a href="https://git.io/blackbox-bandits">this https URL</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Cryptography and Security (cs.CR); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item54">[54]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1808.00441" title="Abstract">arXiv:1808.00441</a> (replaced) [<a href="https://arxiv.org/pdf/1808.00441" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1808.00441" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1808.00441" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Matrix completion and extrapolation via kernel regression
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Gim%C3%A9nez-Febrer%2C+P">Pere Gim&#xe9;nez-Febrer</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Pag%C3%A8s-Zamora%2C+A">Alba Pag&#xe8;s-Zamora</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Giannakis%2C+G+B">Georgios B. Giannakis</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item55">[55]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1808.05298" title="Abstract">arXiv:1808.05298</a> (replaced) [<a href="https://arxiv.org/pdf/1808.05298" title="Download PDF">pdf</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Monitoring through many eyes: Integrating disparate datasets to improve  monitoring of the Great Barrier Reef
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Peterson%2C+E+E">Erin E Peterson</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Santos-Fern%C3%A1ndez%2C+E">Edgar Santos-Fern&#xe1;ndez</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Chen%2C+C">Carla Chen</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Clifford%2C+S">Sam Clifford</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Vercelloni%2C+J">Julie Vercelloni</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Pearse%2C+A">Alan Pearse</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Brown%2C+R">Ross Brown</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Christensen%2C+B">Bryce Christensen</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=James%2C+A">Allan James</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Anthony%2C+K">Ken Anthony</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Loder%2C+J">Jennifer Loder</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Gonz%C3%A1lez-Rivero%2C+M">Manuel Gonz&#xe1;lez-Rivero</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Roelfsema%2C+C">Chris Roelfsema</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Caley%2C+M+J">M.Julian Caley</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Bednarz%2C+T">Tomasz Bednarz</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Mengersen%2C+K">Kerrie Mengersen</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
<dt><a name="item56">[56]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1808.08765" title="Abstract">arXiv:1808.08765</a> (replaced) [<a href="https://arxiv.org/pdf/1808.08765" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1808.08765" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1808.08765" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Identifiability of Complete Dictionary Learning
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Cohen%2C+J+E">J&#xe9;r&#xe9;my E. Cohen</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Gillis%2C+N">Nicolas Gillis</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 19 pages, 2 figures, new title, added references and discussions
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item57">[57]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1809.10243" title="Abstract">arXiv:1809.10243</a> (replaced) [<a href="https://arxiv.org/pdf/1809.10243" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1809.10243" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Segmentation of Skin Lesions and their Attributes Using Multi-Scale  Convolutional Neural Networks and Domain Specific Augmentations
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jahanifar%2C+M">Mostafa Jahanifar</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tajeddin%2C+N+Z">Neda Zamani Tajeddin</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koohbanani%2C+N+A">Navid Alemi Koohbanani</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gooya%2C+A">Ali Gooya</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rajpoot%2C+N">Nasir Rajpoot</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item58">[58]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1811.03433" title="Abstract">arXiv:1811.03433</a> (replaced) [<a href="https://arxiv.org/pdf/1811.03433" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1811.03433" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Explainable cardiac pathology classification on cine MRI with motion  characterization by semi-supervised learning of apparent flow
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng%2C+Q">Qiao Zheng</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Delingette%2C+H">Herv&#xe9; Delingette</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ayache%2C+N">Nicholas Ayache</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item59">[59]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1811.03578" title="Abstract">arXiv:1811.03578</a> (replaced) [<a href="https://arxiv.org/pdf/1811.03578" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1811.03578" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Essential Collaboration Skills: The ASCCR Frame for Collaboration
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Vance%2C+E+A">Eric A. Vance</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Smith%2C+H+S">Heather S. Smith</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 11 pages, 1 figure Made minor changes to V2 and V1 to fix typos, added material in the intro motivating the need for statisticians and data scientists to learn collaboration skills, also added a few more words in other places
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Other Statistics (stat.OT)</span>

</div>
</div>
</dd>
<dt><a name="item60">[60]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1811.12019" title="Abstract">arXiv:1811.12019</a> (replaced) [<a href="https://arxiv.org/pdf/1811.12019" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1811.12019" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Large-Scale Distributed Second-Order Optimization Using  Kronecker-Factored Approximate Curvature for Deep Convolutional Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Osawa%2C+K">Kazuki Osawa</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tsuji%2C+Y">Yohei Tsuji</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ueno%2C+Y">Yuichiro Ueno</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Naruse%2C+A">Akira Naruse</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yokota%2C+R">Rio Yokota</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Matsuoka%2C+S">Satoshi Matsuoka</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 7 figures. Accepted at CVPR 2019, Long Beach, CA
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item61">[61]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1812.02222" title="Abstract">arXiv:1812.02222</a> (replaced) [<a href="https://arxiv.org/pdf/1812.02222" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1812.02222" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Predicting pregnancy using large-scale data from a women&#x27;s health  tracking mobile application
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Shi%2C+S">Shuyang Shi</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wu%2C+Y">Yongshang Wu</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Thomas%2C+D">Daniel Thomas</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Symul%2C+L">Laura Symul</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Pierson%2C+E">Emma Pierson</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Leskovec%2C+J">Jure Leskovec</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Accepted at WWW 2019 (Health on the Web short paper track); an earlier version of this paper was presented at the 2018 NeurIPS ML4H Workshop
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Computers and Society (cs.CY); Machine Learning (cs.LG)

</div>
</div>
</dd>
<dt><a name="item62">[62]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1812.04928" title="Abstract">arXiv:1812.04928</a> (replaced) [<a href="https://arxiv.org/pdf/1812.04928" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1812.04928" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Multiple Model-Free Knockoffs
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Holden%2C+L">Lars Holden</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Hellton%2C+K">Kristoffer Hellton</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 5 pages, 1 figure
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>

</div>
</div>
</dd>
<dt><a name="item63">[63]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.00434" title="Abstract">arXiv:1901.00434</a> (replaced) [<a href="https://arxiv.org/pdf/1901.00434" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1901.00434" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1901.00434" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> The capacity of feedforward neural networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Baldi%2C+P">Pierre Baldi</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vershynin%2C+R">Roman Vershynin</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 49 pages. Introduction is expanded and conclusion is added
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Neural and Evolutionary Computing (cs.NE); Combinatorics (math.CO); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item64">[64]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.02419" title="Abstract">arXiv:1901.02419</a> (replaced) [<a href="https://arxiv.org/pdf/1901.02419" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.02419" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Dynamic Tail Inference with Log-Laplace Volatility
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Chavez%2C+G+V">Gordon V. Chavez</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> New theorem with asymptotic expansions for conditional pdf and large deviation probabilities
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Methodology (stat.ME)</span>; Econometrics (econ.EM); Statistics Theory (math.ST); Risk Management (q-fin.RM); Statistical Finance (q-fin.ST)

</div>
</div>
</dd>
<dt><a name="item65">[65]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.04240" title="Abstract">arXiv:1901.04240</a> (replaced) [<a href="https://arxiv.org/pdf/1901.04240" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.04240" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Semi-supervised Learning with Graphs: Covariance Based Superpixels For  Hyperspectral Image Classification
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sellars%2C+P">Philip Sellars</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aviles-Rivero%2C+A">Angelica Aviles-Rivero</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Papadakis%2C+N">Nicolas Papadakis</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Coomes%2C+D">David Coomes</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Faul%2C+A">Anita Faul</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sch%C3%B6nlieb%2C+C">Carola-Bibane Sch&#xf6;nlieb</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Four pages with two figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Machine Learning (cs.LG); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item66">[66]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.04869" title="Abstract">arXiv:1901.04869</a> (replaced) [<a href="https://arxiv.org/pdf/1901.04869" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.04869" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Optimal acceptance sampling for modules F and F1 of the European  Measuring Instruments Directive
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=M%C3%BCller%2C+C+A">Cord A. M&#xfc;ller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> accepted by J. Appl. Stat. (2019)
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
<dt><a name="item67">[67]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.06523" title="Abstract">arXiv:1901.06523</a> (replaced) [<a href="https://arxiv.org/pdf/1901.06523" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.06523" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Frequency Principle: Fourier Analysis Sheds Light on Deep Neural  Networks
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu%2C+Z+J">Zhi-Qin John Xu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang%2C+Y">Yaoyu Zhang</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo%2C+T">Tao Luo</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao%2C+Y">Yanyang Xiao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma%2C+Z">Zheng Ma</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 5 figures, under review of ICML
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item68">[68]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.07298" title="Abstract">arXiv:1901.07298</a> (replaced) [<a href="https://arxiv.org/pdf/1901.07298" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1901.07298" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1901.07298" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Online Estimation of Multiple Dynamic Graphs in Pattern Sequences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Gaudreault%2C+J">Jimmy Gaudreault</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Saxena%2C+A">Arunabh Saxena</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Shimazaki%2C+H">Hideaki Shimazaki</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 8 pages, 4 figures v2: IJCNN 2019, results unchanged
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (stat.ML)</span>; Disordered Systems and Neural Networks (cond-mat.dis-nn); Machine Learning (cs.LG); Neural and Evolutionary Computing (cs.NE); Neurons and Cognition (q-bio.NC)

</div>
</div>
</dd>
<dt><a name="item69">[69]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.08991" title="Abstract">arXiv:1901.08991</a> (replaced) [<a href="https://arxiv.org/pdf/1901.08991" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.08991" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Diffusion Variational Autoencoders
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rey%2C+L+A+P">Luis A. P&#xe9;rez Rey</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Menkovski%2C+V">Vlado Menkovski</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Portegies%2C+J+W">Jacobus W. Portegies</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 10 pages, 8 figures Added an appendix with derivation of asymptotic expansion of KL divergence for heat kernel on arbitrary Riemannian manifolds, and an appendix with new experiments on binarized MNIST. Added a previously missing factor in the asymptotic expansion of the heat kernel and corrected a coefficient in asymptotic expansion KL divergence; further minor edits
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item70">[70]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1901.10593" title="Abstract">arXiv:1901.10593</a> (replaced) [<a href="https://arxiv.org/pdf/1901.10593" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1901.10593" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Decentralized Online Learning: Take Benefits from Others&#x27; Data without  Sharing Your Own to Track Global Trend
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao%2C+Y">Yawei Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu%2C+C">Chen Yu</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao%2C+P">Peilin Zhao</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu%2C+J">Ji Liu</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Second version: revise Assumption 1 (there is a typo in the first version); add experiments (see Figure 2); revise Algorithm 1 in a more clear way
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item71">[71]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1902.00582" title="Abstract">arXiv:1902.00582</a> (replaced) [<a href="https://arxiv.org/pdf/1902.00582" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1902.00582" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Lower Bounds for Locally Private Estimation via Communication Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Duchi%2C+J">John Duchi</a>, 
<a href="https://arxiv.org/search/math?searchtype=author&amp;query=Rogers%2C+R">Ryan Rogers</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Statistics Theory (math.ST)</span>

</div>
</div>
</dd>
<dt><a name="item72">[72]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1902.08647" title="Abstract">arXiv:1902.08647</a> (replaced) [<a href="https://arxiv.org/pdf/1902.08647" title="Download PDF">pdf</a>, <a href="https://arxiv.org/ps/1902.08647" title="Download PostScript">ps</a>, <a href="https://arxiv.org/format/1902.08647" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Better Algorithms for Stochastic Bandits with Adversarial Corruptions
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta%2C+A">Anupam Gupta</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koren%2C+T">Tomer Koren</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Talwar%2C+K">Kunal Talwar</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item73">[73]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.03223" title="Abstract">arXiv:1903.03223</a> (replaced) [<a href="https://arxiv.org/pdf/1903.03223" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.03223" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Markov-Modulated Hawkes Processes for Sporadic and Bursty Event  Occurrences
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wu%2C+J">Jing Wu</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Zheng%2C+T">Tian Zheng</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Curley%2C+J">James Curley</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
<dt><a name="item74">[74]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.05726" title="Abstract">arXiv:1903.05726</a> (replaced) [<a href="https://arxiv.org/pdf/1903.05726" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.05726" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> A Multi-armed Bandit MCMC, with applications in sampling from doubly  intractable posterior
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Wang%2C+G">Guanyang Wang</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 24 pages, 2 figures
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Computation (stat.CO)</span>; Artificial Intelligence (cs.AI); Data Analysis, Statistics and Probability (physics.data-an); Methodology (stat.ME); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item75">[75]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.07720" title="Abstract">arXiv:1903.07720</a> (replaced) [<a href="https://arxiv.org/pdf/1903.07720" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.07720" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Transfer Entropy Rate Through Lempel-Ziv Complexity
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Restrepo%2C+J+F">Juan F. Restrepo</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Mateos%2C+D+M">Diego M. Mateos</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Schlotthauer%2C+G">Gast&#xf3;n Schlotthauer</a>
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>; Chaotic Dynamics (nlin.CD)

</div>
</div>
</dd>
<dt><a name="item76">[76]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11114" title="Abstract">arXiv:1903.11114</a> (replaced) [<a href="https://arxiv.org/pdf/1903.11114" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11114" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> SUSI: Supervised Self-Organizing Maps for Regression and Classification  in Python
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Riese%2C+F+M">Felix M. Riese</a>, 
<a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Keller%2C+S">Sina Keller</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> Submitted to an ISPRS conference
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Machine Learning (cs.LG)</span>; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (stat.ML)

</div>
</div>
</dd>
<dt><a name="item77">[77]</a>&nbsp;  <span class="list-identifier"><a href="https://arxiv.org/abs/1903.11577" title="Abstract">arXiv:1903.11577</a> (replaced) [<a href="https://arxiv.org/pdf/1903.11577" title="Download PDF">pdf</a>, <a href="https://arxiv.org/format/1903.11577" title="Other formats">other</a>]</span></dt>
<dd>
<div class="meta">
<div class="list-title mathjax">
<span class="descriptor">Title:</span> Statistical Molecule Counting in Super-Resolution Fluorescence  Microscopy: Towards Quantitative Nanoscopy
</div>
<div class="list-authors">
<span class="descriptor">Authors:</span> 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Staudt%2C+T">Thomas Staudt</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Aspelmeier%2C+T">Timo Aspelmeier</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Laitenberger%2C+O">Oskar Laitenberger</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Geisler%2C+C">Claudia Geisler</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Egner%2C+A">Alexander Egner</a>, 
<a href="https://arxiv.org/search/stat?searchtype=author&amp;query=Munk%2C+A">Axel Munk</a>
</div>
<div class="list-comments mathjax">
<span class="descriptor">Comments:</span> 42 pages, 6 figures, 1 table
</div>
<div class="list-subjects">
<span class="descriptor">Subjects:</span> <span class="primary-subject">Applications (stat.AP)</span>

</div>
</div>
</dd>
</dl>
<ul>
<li><a href="https://arxiv.org/list/stat/new?skip=0&amp;show=2000">New submissions</a></li>
<li><a href="#item20">Cross-lists</a></li>
<li><a href="#item47">Replacements</a></li>
</ul>
<small>[ total of 77 entries:  <b>1-77</b>  ]</small><br />
<small>[ showing up to 2000 entries per page:  <a href="https://arxiv.org/list/stat/new?skip=0&amp;show=1000">fewer</a> |  <font color="#999999">more</font> ]</small><br />
</div>
<br/><small><a id="mathjax_toggle" href="javascript:setMathjaxCookie()">Disable MathJax</a> (<a href="https://arxiv.org/help/mathjax/">What is MathJax?</a>)</small><script type="text/javascript" language="javascript">mathjaxToggle();</script>

<hr />
<p>Links to:
<a href="../../index.html" accesskey="a">arXiv</a>, 
<a href="https://arxiv.org/form/stat">form interface</a>,
<a href="https://arxiv.org/find/stat">find</a>,
<a href="../../archive/stat.html">stat</a>, <a href="recent.html">recent</a>, <a href="https://arxiv.org/list/stat/1903">1903</a>,
<a href="../../help/contact.html">contact</a>,
<a href="https://arxiv.org/help/" accesskey="h"><span class="accesskey">h</span>elp</a>&nbsp;
<small>(<a href="https://arxiv.org/help/accesskeys">Access key</a> information)</small>
</p>
<hr />
</div>
</div>
  <footer style="clear: both;">
    <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
      <!-- Macro-Column 1 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="../../about.html">About arXiv</a></li>
              <li><a href="../../about/people/leadership_team.html">Leadership Team</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><span class="icon"><i class="fa fa-envelope"></i></span><a href="../../help/contact.html"> Contact Us</a></li>
              <li><span class="icon"><i class="fa fa-twitter"></i></span><a href="../../../twitter.com/arxiv.html"> Follow us on Twitter</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 1 -->
      <!-- Macro-Column 2 -->
      <div class="column" style="padding: 0;">
        <div class="columns">
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="../../help.html">Help</a></li>
              <li><a href="../../help/policies/privacy_policy.html">Privacy Policy</a></li>
            </ul>
          </div>
          <div class="column">
            <ul style="list-style: none; line-height: 2;">
              <li><a href="https://blogs.cornell.edu/arxiv">Blog</a></li>
              <li><a href="../../help/subscribe.html"> Subscribe</a></li>
            </ul>
          </div>
        </div>
      </div>
      <!-- End Macro-Column 2 -->
    </div>

    <div class="columns" style="border-top: 1px solid #979797; margin: -0.75em;">
      <div class="column">
        <p class="help" style="margin-bottom: 0;">arXiv&#174; is a registered trademark of Cornell University.</p>
      </div>
      <div class="column">
        <p class="help" style="margin-bottom: 0;">If you have a disability and are having trouble accessing information on this website or need materials in an alternate format,
        contact <a href="mailto:web-accessibility@cornell.edu">web-accessibility@cornell.edu</a> for assistance.</p>
      </div>
    </div>
  </footer>
</body>

<!-- Mirrored from arxiv.org/list/stat/new by HTTrack Website Copier/3.x [XR&CO'2014], Fri, 29 Mar 2019 22:40:35 GMT -->
</html>
